import awkward1 as ak
import uproot3
from coffea.nanoevents import NanoEventsFactory, NanoAODSchema
import time
from coffea import processor, hist
from coffea.util import load, save
import json
import glob 
import os
import argparse
import numpy as np
from coffea import lumi_tools


# ---> Class JW Processor
class JW_Processor(processor.ProcessorABC):

	# -- Initializer
	def __init__(self,year,setname,xsec,puweight_arr,corrections):


		lumis = { #Values from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable													  
		#'2016': 35.92,
		#'2017': 41.53,
		'2018': 21.1
		}	

		

		# Parameter set
		self._year = year
		self._lumi = lumis[self._year] * 1000

		self._xsec = xsec
		
		

		# Trigger set
		self._doubleelectron_triggers  ={
			'2018': [
					"Ele23_Ele12_CaloIdL_TrackIdL_IsoVL", # Recomended
					]
		}
	
	
	
		self._singleelectron_triggers = { #2017 and 2018 from monojet, applying dedicated trigger weights
				'2016': [
					'Ele27_WPTight_Gsf',
					'Ele105_CaloIdVT_GsfTrkIdT'
				],
				'2017': [
					'Ele35_WPTight_Gsf',
					'Ele115_CaloIdVT_GsfTrkIdT',
					'Photon200'
				],
				'2018': [
					'Ele32_WPTight_Gsf',   # Recomended
				]
			}
		

		# Corrrection set <-- on developing -->
		self._corrections = corrections
		self._puweight_arr = puweight_arr

		



		# hist set
		self._accumulator = processor.dict_accumulator({

			"sumw": processor.defaultdict_accumulator(float),
	

			"pho_EB_pt": hist.Hist(
				"Events",
				hist.Cat("dataset","Dataset"),
				hist.Bin("pho_EB_pt","Photon EB $P_{T}$ [GeV]", 300, 0, 600),
			),

			"pho_EB_sieie": hist.Hist(
				"Events",
				hist.Cat("dataset","Dataset"),
				hist.Bin("pho_EB_sieie","Photon EB sieie", 100, 0, 0.012),
			),
			

			"pho_EE_pt": hist.Hist(
				"Events",
				hist.Cat("dataset","Dataset"),
				hist.Bin("pho_EE_pt","Photon EE $P_{T}$ [GeV]", 300, 0, 600),
			),

			"pho_EE_sieie": hist.Hist(
				"Events",
				hist.Cat("dataset","Dataset"),
				hist.Bin("pho_EE_sieie","Photon EE sieie", 100, 0, 0.03),
			)
			})
		
	# -- Accumulator: accumulate histograms
	@property
	def accumulator(self):
		return self._accumulator

	# -- Main function : Process events
	def process(self, events):

		# Initialize accumulator
		out = self.accumulator.identity()
		dataset = setname
		#events.metadata['dataset']
		

		isData = 'genWeight' not in events.fields
		

		selection = processor.PackedSelection()

		# Cut flow
		cut0 = np.zeros(len(events))
		

		# --- Selection

		# << flat dim helper function >>
		def flat_dim(arr):

			sub_arr = ak.flatten(arr)
			mask = ~ak.is_none(sub_arr)

			return ak.to_numpy(sub_arr[mask])
		# << drop na helper function >>
		def drop_na(arr):

			mask = ~ak.is_none(arr)

			return arr[mask]
		# << drop na helper function >>
		def drop_na_np(arr):

			mask = ~np.isnan(arr)

			return arr[mask]


		# double lepton trigger
		is_double_ele_trigger=True
		if not is_double_ele_trigger:
			double_ele_triggers_arr=np.ones(len(events), dtype=np.bool)
		else:
			double_ele_triggers_arr = np.zeros(len(events), dtype=np.bool)
			for path in self._doubleelectron_triggers[self._year]:
				if path not in events.HLT.fields: continue
				double_ele_triggers_arr = double_ele_triggers_arr | events.HLT[path]


		# single lepton trigger
		is_single_ele_trigger=True
		if not is_single_ele_trigger:
			single_ele_triggers_arr=np.ones(len(events), dtype=np.bool)
		else:
			single_ele_triggers_arr = np.zeros(len(events), dtype=np.bool)
			for path in self._singleelectron_triggers[self._year]:
				if path not in events.HLT.fields: continue
				single_ele_triggers_arr = single_ele_triggers_arr | events.HLT[path]


		
		Initial_events = events
		print("#### Initial events: ",Initial_events)
		#events = events[single_ele_triggers_arr | double_ele_triggers_arr]
		events = events[double_ele_triggers_arr]
		
		##----------- Cut flow1: Passing Triggers
		cut1 = np.ones(len(events))
		print("#### cut1: ",len(cut1))
		# Particle Identification
		Electron = events.Electron
		Photon   = events.Photon

		Ele_con1 = ak.num(Electron[Electron.cutBased > 2]) > 2
		Ele_con2 = ak.num(Electron[Electron.cutBased==2]) < 1
		#events_c1 = events[Ele_con1 & Ele_con2]
		events_c1 = events[Ele_con1]
		
		print("{0} --> {1}".format(len(events),len(events_c1)))
		
		Electron_c1 = events_c1.Electron
		Photon_c1 = events_c1.Photon		


		
		#@numba.njit ## Numba compile -- Boost!
		def PhotonVID(vid, idBit):
			rBit = 0
			for x in range(0, 7):
				rBit |= (1 << x) if ((vid >> (x * 2)) & 0b11 >= idBit) else 0
			return rBit
		
		
		#@numba.njit ## Numba compile -- Boost!
		def make_fake_obj_mask(Pho):
		
			isFakeObj=[]
			for eventIdx,pho in enumerate(Pho):
				
				NFake=0
				if len(pho) < 1: continue;
				for phoIdx,_ in enumerate(pho):
					vid = Pho[eventIdx][phoIdx].vidNestedWPBitmap
					vid_cuts1 = PhotonVID(vid,1) # Loose photon
					vid_cuts2 = PhotonVID(vid,2) # Medium photon
					vid_cuts3 = PhotonVID(vid,3) # Tight photon
		
					# Field name
					# |0|0|0|0|0|0|0| 
					# |IsoPho|IsoNeu|IsoChg|Sieie|hoe|scEta|PT|
					
					# 1. Turn off cut (ex turn off Sieie
					# |1|1|1|0|1|1|1| = |1|1|1|0|1|1|1|
					
					# 2. Inverse cut (ex inverse Sieie)
					# |1|1|1|1|1|1|1| = |1|1|1|0|1|1|1|
					
					
					#if (vid_cuts2 & 0b1101111 == 0b1101111): # without Isochg
					if (vid_cuts2 & 0b1111111 == 0b1110111) & (~Pho[eventIdx][phoIdx].pixelSeed): # Inverse Sieie
					#if (vid_cuts2 & 0b1110111 == 0b1110111): # Without Sieie
						
						if Pho[eventIdx][phoIdx].isScEtaEB:
							if Pho[eventIdx][phoIdx].sieie < 0.01015 * 1.75:
								NFake+=1
						
						elif Pho[eventIdx][phoIdx].isScEtaEE:
							if Pho[eventIdx][phoIdx].sieie < 0.0272 * 1.75:
								NFake+=1
		
						else: continue;
							
							
				if NFake == 1:
					isFakeObj.append(True)
				else:
					isFakeObj.append(False)
				
			return isFakeObj
		
		
		start = time.time()
		isFakeObj = make_fake_obj_mask(Photon_c1)
		print(time.time() - start)
		
		
		def make_IsochgSide_mask(Pho):
		
			isIsoSideObj=[]
			for eventIdx,pho in enumerate(Pho):
				
				NFake=0
				if len(pho) < 1: continue;
				for phoIdx,_ in enumerate(pho):
					vid = Pho[eventIdx][phoIdx].vidNestedWPBitmap
					vid_cuts1 = PhotonVID(vid,1) # Loose photon
					vid_cuts2 = PhotonVID(vid,2) # Medium photon
					vid_cuts3 = PhotonVID(vid,3) # Tight photon
					
					
					if (vid_cuts2 & 0b1111111 == 0b1101111): # without Isochg
						isochg = Pho[eventIdx][phoIdx].pfRelIso03_chg
						#if (isochg > 4) & (isochg < 10):
						NFake+=1
							
				if NFake == 1:
					isIsoSideObj.append(True)
				else:
					isIsoSideObj.append(False)
				
			return isIsoSideObj
		
		
		start = time.time()
		isIsoSideObj = make_IsochgSide_mask(Photon_c1)
		print(time.time() - start)
		
		
		isOneMed_pho =  ak.num(Photon_c1[Photon_c1.cutBased > 1]) == 1
		events_data_template  = events_c1[isOneMed_pho | np.array(isFakeObj)]
		events_fake_template = events_c1[np.array(isIsoSideObj) | np.array(isFakeObj)]

		FakePhoton = events_fake_template.Photon
		isEE_mask = FakePhoton.isScEtaEE
		isEB_mask = FakePhoton.isScEtaEB

		FakePhoton_EE = FakePhoton[isEE_mask]
		FakePhoton_EB = FakePhoton[isEB_mask]

		Pho_EB_PT	= flat_dim(FakePhoton_EB.pt)
		Pho_EB_sieie = flat_dim(FakePhoton_EB.sieie)
		Pho_EE_PT	= flat_dim(FakePhoton_EE.pt)
		Pho_EE_sieie = flat_dim(FakePhoton_EE.sieie)
		
		# Initial events
		out["sumw"][dataset] += len(Initial_events)
		out["pho_EE_pt"].fill(
			dataset=dataset,
			pho_EE_pt=Pho_EE_PT,
		)
		out["pho_EE_sieie"].fill(
			dataset=dataset,
			pho_EE_sieie=Pho_EE_sieie,
		)
		out["pho_EB_pt"].fill(
			dataset=dataset,
			pho_EB_pt=Pho_EB_PT,
		)
		out["pho_EB_sieie"].fill(
			dataset=dataset,
			pho_EB_sieie=Pho_EB_sieie,
		)

		return out

	# -- Finally! return accumulator
	def postprocess(self,accumulator):
		return accumulator
# <---- Class JW_Processor


if __name__ == '__main__':

	start = time.time()
	parser = argparse.ArgumentParser()
	
	parser.add_argument('--nWorker', type=int,
				help=" --nWorker 2", default=20)
	parser.add_argument('--metadata', type=str,
				help="--metadata xxx.json")
	parser.add_argument('--dataset', type=str,
				help="--dataset ex) Egamma_Run2018A_280000")
	args = parser.parse_args()
	
	
	## Prepare files
	N_node = args.nWorker
	metadata = args.metadata
	data_sample = args.dataset
	year='2018'
	xsecDY=2137.0

	## Json file reader
	with open(metadata) as fin:
		datadict = json.load(fin)



	filelist = glob.glob(datadict[data_sample])

	sample_name = data_sample.split('_')[0]
	setname = metadata.split('.')[0].split('/')[1]
	
	
	## Read Correction file <-- on developing -->
	#corr_file = "../Corrections/corrections.coffea"
	corr_file = "corrections.coffea"
	corrections = load(corr_file)

	## Read PU weight file
	#with open('../Corrections/Pileup/puWeight/pu_weight_RunAB.npy','rb') as f:
	with open('pu_weight_RunAB.npy','rb') as f:
		pu = np.load(f)


#	# test one file 
#	sample_name="DY"
#	setname="DY"
#	filelist=["/x6/cms/store_skim_2ElIdPt20/mc/RunIISummer19UL18NanoAODv2/DYToEE_M-50_NNPDF31_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v15_L1v1-v1/280000/59AB328B-F0E3-F544-98BB-E5E55577C649_skim_2ElIdPt20.root"]
#



	print(sample_name)
	samples = {
		sample_name : filelist
	}
	
	
	# Class -> Object
	#JW_Processor_instance = JW_Processor(year,setname,corrections,xsecDY)  <--on developing-->
	JW_Processor_instance = JW_Processor(year,setname,xsecDY,pu,corrections)
	
	
	## -->Multi-node Executor
	result = processor.run_uproot_job(
		samples,  #dataset
		"Events", # Tree name
		JW_Processor_instance, # Class
		executor=processor.futures_executor,
		executor_args={"schema": NanoAODSchema, "workers": 20},
	#maxchunks=4,
	)
	
	outname = data_sample + '.futures'
	#outname = 'DY_test.futures'
	save(result,outname)
	
	elapsed_time = time.time() - start
	print("Time: ",elapsed_time)
